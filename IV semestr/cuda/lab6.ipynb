{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNw++mER9OifDdw9wg6wPVD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!nvcc --version"],"metadata":{"id":"60wNgcil3lRx","executionInfo":{"status":"ok","timestamp":1748437772222,"user_tz":-120,"elapsed":157,"user":{"displayName":"Kacper Szczudło","userId":"12561650817493908318"}},"outputId":"d4c20236-421c-4802-b861-2ef473dddc22","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2024 NVIDIA Corporation\n","Built on Thu_Jun__6_02:18:23_PDT_2024\n","Cuda compilation tools, release 12.5, V12.5.82\n","Build cuda_12.5.r12.5/compiler.34385749_0\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"O50Zk_Bz_qh4","executionInfo":{"status":"ok","timestamp":1748437772296,"user_tz":-120,"elapsed":76,"user":{"displayName":"Kacper Szczudło","userId":"12561650817493908318"}},"outputId":"7934f124-d5d5-43a2-f8ad-775c0c6b89ae","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed May 28 13:09:32 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"FkjIr5ib1xkK","executionInfo":{"status":"ok","timestamp":1748437772318,"user_tz":-120,"elapsed":21,"user":{"displayName":"Kacper Szczudło","userId":"12561650817493908318"}},"outputId":"50ea6c70-c9eb-493f-966d-f0a99d420d7f","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing matrix_multiply_2d.cu\n"]}],"source":["%%writefile matrix_multiply_2d.cu\n","#include <stdio.h>\n","#include <cuda_runtime.h>\n","#include <stdlib.h>\n","\n","#define TILE_WIDTH 16 // Rozmiar kafelka (tile size)\n","\n","void checkCudaError(cudaError_t err, const char *msg) {\n","    if (err != cudaSuccess) {\n","        printf(\"Błąd CUDA w %s: %s\\n\", msg, cudaGetErrorString(err));\n","        exit(-1);\n","    }\n","}\n","\n","__global__ void matrix_multiply_shared(float *Md, float *Nd, float *Pd, int width) {\n","    __shared__ float Mds[TILE_WIDTH][TILE_WIDTH];\n","    __shared__ float Nds[TILE_WIDTH][TILE_WIDTH];\n","\n","    int row = blockIdx.y * TILE_WIDTH + threadIdx.y;\n","    int col = blockIdx.x * TILE_WIDTH + threadIdx.x;\n","    float pValue = 0.0f;\n","\n","    for (int k = 0; k < (width + TILE_WIDTH - 1) / TILE_WIDTH; k++) {\n","        if (row < width && (k * TILE_WIDTH + threadIdx.x) < width) {\n","            Mds[threadIdx.y][threadIdx.x] = Md[row * width + (k * TILE_WIDTH + threadIdx.x)];\n","        } else {\n","            Mds[threadIdx.y][threadIdx.x] = 0.0f;\n","        }\n","\n","        if (col < width && (k * TILE_WIDTH + threadIdx.y) < width) {\n","            Nds[threadIdx.y][threadIdx.x] = Nd[(k * TILE_WIDTH + threadIdx.y) * width + col];\n","        } else {\n","            Nds[threadIdx.y][threadIdx.x] = 0.0f;\n","        }\n","\n","        __syncthreads();\n","\n","        for (int i = 0; i < TILE_WIDTH; i++) {\n","            pValue += Mds[threadIdx.y][i] * Nds[i][threadIdx.x];\n","        }\n","\n","        __syncthreads();\n","    }\n","\n","    if (row < width && col < width) {\n","        Pd[row * width + col] = pValue;\n","    }\n","}\n","\n","void printMatrix(float *matrix, int N) {\n","    for (int i = 0; i < N; i++) {\n","        for (int j = 0; j < N; j++) {\n","            printf(\"%8.2f \", matrix[i * N + j]);\n","        }\n","        printf(\"\\n\");\n","    }\n","}\n","\n","int main() {\n","    int N;\n","    char input_choice, display_choice;\n","\n","    printf(\"Podaj rozmiar macierzy (N): \");\n","    scanf(\"%d\", &N);\n","    if (N <= 0) {\n","        printf(\"Rozmiar macierzy musi być dodatni!\\n\");\n","        return 1;\n","    }\n","\n","    printf(\"Czy chcesz ręcznie wprowadzić macierze M i N? (t/n): \");\n","    scanf(\" %c\", &input_choice);\n","\n","    int size = N * N * sizeof(float);\n","    float *h_M = (float *)malloc(size);\n","    float *h_N = (float *)malloc(size);\n","    float *h_P = (float *)malloc(size);\n","    if (!h_M || !h_N || !h_P) {\n","        printf(\"Błąd alokacji pamięci na hoście!\\n\");\n","        return 1;\n","    }\n","\n","    for (int i = 0; i < N * N; i++) {\n","        h_P[i] = 0.0f;\n","    }\n","\n","    if (input_choice == 't' || input_choice == 'T') {\n","        printf(\"Wprowadź %d elementów macierzy M (wierszami):\\n\", N * N);\n","        for (int i = 0; i < N * N; i++) {\n","            scanf(\"%f\", &h_M[i]);\n","        }\n","        printf(\"Wprowadź %d elementów macierzy N (wierszami):\\n\", N * N);\n","        for (int i = 0; i < N * N; i++) {\n","            scanf(\"%f\", &h_N[i]);\n","        }\n","    } else {\n","        printf(\"Generowanie macierzy M i N z wartościami 1 do %d\\n\", N * N);\n","        for (int i = 0; i < N * N; i++) {\n","            h_M[i] = (float)(i + 1);\n","            h_N[i] = (float)(i + 1);\n","        }\n","    }\n","\n","    printf(\"Czy wyświetlić macierze wejściowe (M, N) i wyjściową (P)? (t/n): \");\n","    scanf(\" %c\", &display_choice);\n","\n","    if (display_choice == 't' || display_choice == 'T') {\n","        printf(\"\\nMacierz M (wejściowa):\\n\");\n","        printMatrix(h_M, N);\n","        printf(\"\\nMacierz N (wejściowa):\\n\");\n","        printMatrix(h_N, N);\n","    }\n","\n","    float *d_M, *d_N, *d_P;\n","    checkCudaError(cudaMalloc(&d_M, size), \"cudaMalloc d_M\");\n","    checkCudaError(cudaMalloc(&d_N, size), \"cudaMalloc d_N\");\n","    checkCudaError(cudaMalloc(&d_P, size), \"cudaMalloc d_P\");\n","\n","    checkCudaError(cudaMemcpy(d_M, h_M, size, cudaMemcpyHostToDevice), \"cudaMemcpy d_M\");\n","    checkCudaError(cudaMemcpy(d_N, h_N, size, cudaMemcpyHostToDevice), \"cudaMemcpy d_N\");\n","\n","    dim3 threadsPerBlock(TILE_WIDTH, TILE_WIDTH);\n","    dim3 blocksPerGrid((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n","                       (N + threadsPerBlock.y - 1) / threadsPerBlock.y);\n","\n","    printf(\"\\nLiczba bloków w siatce: (%d, %d)\\n\", blocksPerGrid.x, blocksPerGrid.y);\n","    printf(\"Liczba wątków w bloku: (%d, %d)\\n\", threadsPerBlock.x, threadsPerBlock.y);\n","\n","    matrix_multiply_shared<<<blocksPerGrid, threadsPerBlock>>>(d_M, d_N, d_P, N);\n","    checkCudaError(cudaGetLastError(), \"uruchomienie kernela\");\n","\n","    checkCudaError(cudaDeviceSynchronize(), \"synchronizacja urządzenia\");\n","\n","    checkCudaError(cudaMemcpy(h_P, d_P, size, cudaMemcpyDeviceToHost), \"cudaMemcpy d_P\");\n","\n","    if (display_choice == 't' || display_choice == 'T') {\n","        printf(\"\\nMacierz P (wyjściowa, M * N):\\n\");\n","        printMatrix(h_P, N);\n","    }\n","\n","    checkCudaError(cudaFree(d_M), \"cudaFree d_M\");\n","    checkCudaError(cudaFree(d_N), \"cudaFree d_N\");\n","    checkCudaError(cudaFree(d_P), \"cudaFree d_P\");\n","\n","    free(h_M);\n","    free(h_N);\n","    free(h_P);\n","\n","    return 0;\n","}"]},{"cell_type":"code","source":["!nvcc -arch=sm_75 matrix_multiply_2d.cu -o matrix_multiply_2d"],"metadata":{"id":"_Q7VqlQJ2XHh","executionInfo":{"status":"ok","timestamp":1748437776485,"user_tz":-120,"elapsed":4166,"user":{"displayName":"Kacper Szczudło","userId":"12561650817493908318"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!./matrix_multiply_2d"],"metadata":{"id":"glFk8s9U2YTo","executionInfo":{"status":"ok","timestamp":1748437785684,"user_tz":-120,"elapsed":9201,"user":{"displayName":"Kacper Szczudło","userId":"12561650817493908318"}},"outputId":"404627b3-56c6-4b82-c8c5-97f4a6adf84f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Podaj rozmiar macierzy (N): 2\n","Czy chcesz ręcznie wprowadzić macierze M i N? (t/n): n\n","Generowanie macierzy M i N z wartościami 1 do 4\n","Czy wyświetlić macierze wejściowe (M, N) i wyjściową (P)? (t/n): t\n","\n","Macierz M (wejściowa):\n","    1.00     2.00 \n","    3.00     4.00 \n","\n","Macierz N (wejściowa):\n","    1.00     2.00 \n","    3.00     4.00 \n","\n","Liczba bloków w siatce: (1, 1)\n","Liczba wątków w bloku: (16, 16)\n","\n","Macierz P (wyjściowa, M * N):\n","    7.00    10.00 \n","   15.00    22.00 \n"]}]}]}